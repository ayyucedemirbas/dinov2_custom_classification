{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNX/Uik/quCgG90F24NiWIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/dinov2_custom_classification/blob/main/dinov2_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/dinov2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54L_dPM7uO7m",
        "outputId": "1bc6b928-f540-43b1-b24e-b41974e7b817"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dinov2'...\n",
            "remote: Enumerating objects: 485, done.\u001b[K\n",
            "remote: Counting objects: 100% (260/260), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 485 (delta 172), reused 160 (delta 160), pack-reused 225\u001b[K\n",
            "Receiving objects: 100% (485/485), 1.11 MiB | 21.42 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd dinov2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fAkzqfouVxc",
        "outputId": "ba2fcfa0-7002-4de1-959f-83851da7b54e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dinov2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dinov2.models.vision_transformer import vit_small, vit_base, vit_large, vit_giant2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vzusbIqzDGg",
        "outputId": "556a6e4f-c7b6-4441-eefe-f365ea7d89a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/dinov2/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/content/dinov2/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/content/dinov2/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple notebook demonstrating how to fine-tune a DinoV2 classifier on your own images/labels\n",
        "\n",
        "# Most of the core code was originally published in an excellent tutorial by Kili Technology, here:\n",
        "#  https://colab.research.google.com/github/kili-technology/kili-python-sdk/blob/main/recipes/finetuning_dinov2.ipynb\n",
        "\n",
        "# November 11th, 2023 by Lance Legel (lance@3co.ai) from 3co, Inc. (https://3co.ai)\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch import nn, optim\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "from torchvision import datasets, transforms\n"
      ],
      "metadata": {
        "id": "oVBQ-G9duhCg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CKtwDE7ZtaDE"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"username\",\"key\":\"key\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ni6Q500x5r3",
        "outputId": "6dc4a9b9-cd89-4d1f-913e-ddacd1799a69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfQRxjQytzuN",
        "outputId": "211da919-b1d7-4028-ff85-8d6b47f6f4c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading brain-tumor-mri-dataset.zip to /content\n",
            " 99% 147M/149M [00:09<00:00, 18.9MB/s]\n",
            "100% 149M/149M [00:09<00:00, 15.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq brain-tumor-mri-dataset.zip"
      ],
      "metadata": {
        "id": "wHOGGr9_t0du"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_dir = Path(\"Training\") # in \"train\", put a folder for each class, with folder name = class name\n",
        "valid_dataset_dir = Path(\"Testing\")   # in \"val\", same as above"
      ],
      "metadata": {
        "id": "LlJew_omul-i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are settings for ensuring input images to DinoV2 are properly sized\n",
        "\n",
        "class ResizeAndPad:\n",
        "    def __init__(self, target_size, multiple):\n",
        "        self.target_size = target_size\n",
        "        self.multiple = multiple\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Resize the image\n",
        "        img = transforms.Resize(self.target_size)(img)\n",
        "\n",
        "        # Calculate padding\n",
        "        pad_width = (self.multiple - img.width % self.multiple) % self.multiple\n",
        "        pad_height = (self.multiple - img.height % self.multiple) % self.multiple\n",
        "\n",
        "        # Apply padding\n",
        "        img = transforms.Pad((pad_width // 2, pad_height // 2, pad_width - pad_width // 2, pad_height - pad_height // 2))(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "image_dimension = 256\n",
        "\n",
        "# This is what DinoV2 sees\n",
        "target_size = (image_dimension, image_dimension)\n",
        "\n",
        "# Below are functions that every image will be passed through, including data augmentations\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            ResizeAndPad(target_size, 14),\n",
        "            transforms.RandomRotation(360),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Here is where we wrap up our images, which are in folders (defined above) where the folder name is class name\n",
        "# As long as you defined the \"train\" folder above with sub-folders for each class, below will \"just work\"\n",
        "image_datasets = {\n",
        "    \"train\": datasets.ImageFolder(train_dataset_dir, data_transforms[\"train\"])\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets[\"train\"], batch_size=8, shuffle=True)\n",
        "}\n",
        "\n",
        "class_names = image_datasets[\"train\"].classes"
      ],
      "metadata": {
        "id": "aeUzYpYRuqdr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8HtmIJAZU_0",
        "outputId": "f4cc0e72-861a-4f2a-8ecc-d45d7ad3b116"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['glioma', 'meningioma', 'notumor', 'pituitary']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "EW2PpQpBuzWq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVzzAzOJZcQU",
        "outputId": "526984d8-456c-418a-d0bd-5bfba2a7d739"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkb5y5Lwvv2Q",
        "outputId": "ae01734c-3ea7-4a9f-8d19-feb340c88773"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-19 20:27:59--  https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.102, 108.157.254.15, 108.157.254.124, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88291785 (84M) [binary/octet-stream]\n",
            "Saving to: ‘dinov2_vits14_reg4_pretrain.pth’\n",
            "\n",
            "dinov2_vits14_reg4_ 100%[===================>]  84.20M   275MB/s    in 0.3s    \n",
            "\n",
            "2024-03-19 20:28:00 (275 MB/s) - ‘dinov2_vits14_reg4_pretrain.pth’ saved [88291785/88291785]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new classifier layer that contains a few linear layers with a ReLU to make predictions positive\n",
        "class DinoVisionTransformerClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, model_size=\"small\"):\n",
        "        super(DinoVisionTransformerClassifier, self).__init__()\n",
        "        self.model_size = model_size\n",
        "\n",
        "        # loading a model with registers\n",
        "        n_register_tokens = 4\n",
        "\n",
        "        if model_size == \"small\":\n",
        "            model = vit_small(patch_size=14,\n",
        "                              img_size=526,\n",
        "                              init_values=1.0,\n",
        "                              num_register_tokens=n_register_tokens,\n",
        "                              block_chunks=0)\n",
        "            self.embedding_size = 384\n",
        "            self.number_of_heads = 6\n",
        "\n",
        "        elif model_size == \"base\":\n",
        "            model = vit_base(patch_size=14,\n",
        "                             img_size=526,\n",
        "                             init_values=1.0,\n",
        "                             num_register_tokens=n_register_tokens,\n",
        "                             block_chunks=0)\n",
        "            self.embedding_size = 768\n",
        "            self.number_of_heads = 12\n",
        "\n",
        "        elif model_size == \"large\":\n",
        "            model = vit_large(patch_size=14,\n",
        "                              img_size=526,\n",
        "                              init_values=1.0,\n",
        "                              num_register_tokens=n_register_tokens,\n",
        "                              block_chunks=0)\n",
        "            self.embedding_size = 1024\n",
        "            self.number_of_heads = 16\n",
        "\n",
        "        elif model_size == \"giant\":\n",
        "            model = vit_giant2(patch_size=14,\n",
        "                               img_size=526,\n",
        "                               init_values=1.0,\n",
        "                               num_register_tokens=n_register_tokens,\n",
        "                               block_chunks=0)\n",
        "            self.embedding_size = 1536\n",
        "            self.number_of_heads = 24\n",
        "\n",
        "        # Download pre-trained weights and place locally as-needed:\n",
        "        # - small: https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth\n",
        "        # - base:  https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth\n",
        "        # - large: https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth\n",
        "        # - giant: https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth\n",
        "        model.load_state_dict(torch.load(Path('/content/dinov2_vits14_reg4_pretrain.pth')))\n",
        "\n",
        "        self.transformer = deepcopy(model)\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.Linear(self.embedding_size, 256), nn.ReLU(), nn.Linear(256, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transformer(x)\n",
        "        x = self.transformer.norm(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = DinoVisionTransformerClassifier(\"small\")"
      ],
      "metadata": {
        "id": "IzyYi_kEu1kT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model = model.train()"
      ],
      "metadata": {
        "id": "Fb4Le-gevB2U"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change the binary cross-entropy loss below to a different loss if using more than 2 classes\n",
        "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6)"
      ],
      "metadata": {
        "id": "5u9Yeg8BvItY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "6Xb3rTnHvfaF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(outputs, labels):\n",
        "    # Convert outputs to probabilities using sigmoid\n",
        "    probabilities = torch.sigmoid(outputs)\n",
        "    # Convert probabilities to predicted classes\n",
        "    predicted_classes = probabilities > 0.5\n",
        "    # Calculate accuracy\n",
        "    correct_predictions = (predicted_classes == labels.byte()).sum().item()\n",
        "    total_predictions = labels.size(0)\n",
        "    return correct_predictions / total_predictions\n",
        "\n",
        "epoch_losses = []\n",
        "epoch_accuracies = []\n",
        "\n",
        "print(\"Training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    batch_losses = []\n",
        "    batch_accuracies = []\n",
        "\n",
        "    for data in dataloaders[\"train\"]:\n",
        "        # get the input batch and the labels\n",
        "        batch_of_images, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # model prediction\n",
        "        output = model(batch_of_images.to(device)).squeeze(dim=1)\n",
        "\n",
        "        # compute loss and do gradient descent\n",
        "        loss = criterion(output, labels.float().to(device))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss.item())\n",
        "\n",
        "        # Calculate and record batch accuracy\n",
        "        accuracy = calculate_accuracy(output, labels.to(device))\n",
        "        batch_accuracies.append(accuracy)\n",
        "\n",
        "    epoch_losses.append(np.mean(batch_losses))\n",
        "    epoch_accuracy = np.mean(batch_accuracies)\n",
        "    epoch_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(\"  -> Epoch {}: Loss = {:.5f}, Accuracy = {:.3f}%\".format(epoch, epoch_losses[-1], 100*epoch_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5SrbmEnvljy",
        "outputId": "63bfb566-52b6-48ac-9209-b881438d1452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "  -> Epoch 0: Loss = 23.66168, Accuracy = 32.300%\n",
            "  -> Epoch 1: Loss = 22.82400, Accuracy = 40.616%\n",
            "  -> Epoch 2: Loss = 22.62972, Accuracy = 42.665%\n",
            "  -> Epoch 3: Loss = 22.51077, Accuracy = 43.452%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_directory = os.getcwd()"
      ],
      "metadata": {
        "id": "uTdsO0CWa6Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch_accuracies, label='Accuracy', color='blue')\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"Epoch Number\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Plotting loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch_losses, label='Loss', color='red')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch Number\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "97upOBSVaDEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# During inference / testing / deployment, we want to remove data augmentations from the input transform:\n",
        "inference_preprocessing = transforms.Compose([ ResizeAndPad(target_size, 14),\n",
        "                                               transforms.ToTensor(),\n",
        "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "                                             ]\n",
        "                                            )"
      ],
      "metadata": {
        "id": "ro3Jq-SQaLDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_on_validation_data(inference_model, n_test_images=1):\n",
        "\n",
        "    for class_name in class_names:\n",
        "        # gather N validation images per class\n",
        "        val_class_dir = \"{}/{}\".format(valid_dataset_dir, class_name)\n",
        "        val_test_images = os.listdir(val_class_dir)[:n_test_images]\n",
        "\n",
        "        for val_test_image in val_test_images:\n",
        "\n",
        "            if \".jpg\" not in val_test_image and \".png\" not in val_test_image:\n",
        "                continue\n",
        "\n",
        "            # load image\n",
        "            img = Image.open(\"{}/{}\".format(val_class_dir, val_test_image))\n",
        "\n",
        "            # pre-process image and load onto device\n",
        "            img_tensor = inference_preprocessing(img)\n",
        "            img_tensor = img_tensor.unsqueeze(0)\n",
        "            input_tensor = img_tensor.to(device)\n",
        "\n",
        "            # run model on input image data\n",
        "            with torch.no_grad():\n",
        "                embeddings = inference_model.transformer(input_tensor)\n",
        "                x = inference_model.transformer.norm(embeddings)\n",
        "                output_tensor = inference_model.classifier(x)\n",
        "\n",
        "                # sigmoid\n",
        "                score = output_tensor[0][0].item()\n",
        "                score = 1 / (1 + np.exp(-score))\n",
        "\n",
        "                # for binary classification, we can just interpret class name based on the 1 output score\n",
        "                predicted_class = class_names[1] if score > 0.5 else class_names[0]\n",
        "\n",
        "                print(\"\\n\\n\\nDinoV2 Predicted = {} vs. Ground Truth = {}:\".format(predicted_class, class_name))\n",
        "                plt.imshow(img)\n",
        "                plt.show()"
      ],
      "metadata": {
        "id": "bXO3Ej4jaWXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), '{}/classifier.pth'.format(local_directory))"
      ],
      "metadata": {
        "id": "nuYTlR_0aeRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reloading the model with trained classification weights to demonstrate deployment\n",
        "classifier = DinoVisionTransformerClassifier(\"small\")\n",
        "\n",
        "classifier.load_state_dict(torch.load('{}/classifier.pth'.format(local_directory)))\n",
        "\n",
        "classifier = classifier.to(device)\n",
        "classifier.eval()"
      ],
      "metadata": {
        "id": "if7keaylbJzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_on_validation_data(inference_model=classifier, n_test_images=1)"
      ],
      "metadata": {
        "id": "1E6m7AsUbXJ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}